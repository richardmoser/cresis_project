{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-12T21:14:41.220615Z",
     "start_time": "2024-08-12T21:14:40.067002Z"
    }
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from project_classes import *\n",
    "from functions import *\n",
    "# from iceflow_library import *\n",
    "# import scipy.optimize as optimize\n",
    "# from scipy.optimize import curve_fit\n",
    "print(\"Imports successful.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports successful.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T21:14:41.227068Z",
     "start_time": "2024-08-12T21:14:41.222820Z"
    }
   },
   "cell_type": "code",
   "source": [
    "zoom = True\n",
    "seg_length = 250\n",
    "# season = \"2009_Antarctica_DC8\"\n",
    "# season1 = \"2018_Antarctica_DC8\" \n",
    "# season2 = \"2018_Antarctica_DC8\" \n",
    "# season1 = \"2016_Antarctica_DC8\" \n",
    "# season2 = \"2016_Antarctica_DC8\" \n",
    "# season = \"2014_Antarctica_DC8\" \n",
    "# season1 = \"2022_Antarctica_BaslerMKB\"\n",
    "season = \"2022_Antarctica_BaslerMKB\"\n",
    "# season1 = \"2013_Antarctica_P3\"\n",
    "\n",
    "# file_name1 = \"C:\\\\Users\\\\moser\\\\Desktop\\\\cresis_project\\\\pickle_jar\\\\layer_export_\" + flight1 + \".pickle\"\n",
    "# file_name2 = \"C:\\\\Users\\\\moser\\\\Desktop\\\\cresis_project\\\\pickle_jar\\\\layer_export_\" + flight2 + \".pickle\"\n",
    "testing = False"
   ],
   "id": "5a77c79d4ba30b55",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T21:20:35.177593Z",
     "start_time": "2024-08-12T21:20:35.157662Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def mat_pickler_h5py(season, flight, testing_mode=False, readout=False, save=True, plot_layer=False, convert_xy=False):\n",
    "    # print(\"Reading data files...\")\n",
    "    # print(section_break)\n",
    "    # set the directory, segment data file, layer attributes file, and start and end frames\n",
    "    if testing_mode:\n",
    "        # the dir is the current directory + the test_data folder\n",
    "        dir = os.getcwd() + '\\\\test_data\\\\' + flight + '\\\\'\n",
    "    else:\n",
    "        # TODO: refactor this into a try except block, maybe upstream of this function where it is called\n",
    "        dir = ('C:\\\\Users\\\\moser\\\\Documents\\\\cresis\\\\rds\\\\' + season + '\\\\CSARP_layer\\\\' + flight + '\\\\')\n",
    "\n",
    "    data_file = dir + 'Data_' + flight + '_'\n",
    "    attributes_file = dir + 'layer_' + flight\n",
    "    layers = layerize_h5py(data_file, attributes_file, dir, convert_xy)\n",
    "    if layers == True:\n",
    "        return True\n",
    "\n",
    "    if readout:\n",
    "        print(section_break, end=\"\")\n",
    "        for layer in layers:\n",
    "            print(f\"\\n{layer.layer_name} number of points: {layer.twtt.shape[0]}\")\n",
    "            print(f\"{layer.layer_name} twtt first three: {layer.twtt[:3].tolist()} \")\n",
    "            print(f\"{layer.layer_name} twtt last three: {layer.twtt[-3:].tolist()} \")\n",
    "        print(section_break + \"\\n\")\n",
    "\n",
    "    if save:\n",
    "        # save layers to a pickle file\n",
    "        # print(\"Saving layers to a pickle file...\")\n",
    "        # print(section_break)\n",
    "        # list current directory\n",
    "        # print(f\"Current directory: {}\")\n",
    "        directory = os.getcwd() + \"\\\\pickle_jar\\\\\"\n",
    "        # file_name = directory + \"layer_export\" + attributes_file[5:-4] + \".pickle\"\n",
    "        file_name = directory + \"layer_export_\" + flight + \".pickle\"\n",
    "        pickle.dump(layers, open(file_name, \"wb\"))\n",
    "        # print(file_name, \" saved in local directory of this python file.\")\n",
    "        # print(section_break + \"\\n\")\n",
    "\n",
    "    if plot_layer:\n",
    "        # plot the layers\n",
    "        print(\"Plotting layers...\")\n",
    "        print(section_break)\n",
    "        # plot the layer depths vs gps time for each layer on the same plot\n",
    "        for layer in layers:\n",
    "            plt.plot(layer.gps_time, layer.twtt, label=layer.layer_name)\n",
    "        plt.xlabel(\"GPS Time\")\n",
    "        plt.ylabel(\"Two Way Travel Time (ns)\")\n",
    "        # invert the y axis so that the plot is right side up\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.title(\"Elevation vs GPS Time\")\n",
    "        plt.legend()\n",
    "\n",
    "        plt.show()\n",
    "        print(section_break + \"\\n\")\n",
    "\n",
    "\n",
    "def layerize_h5py(data_file, attribute_file, dir, convert_xy):\n",
    "    # TODO: add a docstring\n",
    "    # TODO: add a portion to pull the layer name out of the attribute file\n",
    "    # afai can tell, that is all the attribute file is used for in the original code\n",
    "    # create an empty list to store the layers\n",
    "    layers = []\n",
    "    files = os.listdir(dir)  # list of files in the directory\n",
    "    # TODO: shift to differentiating data vs layer based on file name. Data starts with \"Data_\" and layer starts with \"layer_\"\n",
    "    startframe = '001'  # the first file number in the directory\n",
    "    endframe = str(len(files) - 1).zfill(3)  # the number of files in the directory\n",
    "    # debug_print(f\"attribute_file: {attribute_file}\")\n",
    "    attribute_mat = h5py.File(attribute_file + '.mat', 'r')\n",
    "    # print the keys in the attribute file\n",
    "\n",
    "    decimal1 = attribute_mat[np.array([attribute_mat['lyr_name'][0][0]])[0]]\n",
    "    decimal1_name = [decimal1[i][0] for i in range(len(decimal1))]  # a list of unicode values of the characters\n",
    "    decimal2 = attribute_mat[np.array([attribute_mat['lyr_name'][1][0]])[0]]\n",
    "    decimal2_name = [decimal2[i][0] for i in range(len(decimal2))]  # a list of unicode values of the characters\n",
    "    name1 = ''.join(chr(i) for i in decimal1_name)  # convert the unicode values to a string\n",
    "    name2 = ''.join(chr(i) for i in decimal2_name)  # convert the unicode values to a string\n",
    "\n",
    "    layer1_name = name1\n",
    "    layer2_name = name2\n",
    "\n",
    "    # print(f\"data_file: {data_file}\")\n",
    "    filled_data_file = data_file + str(1).zfill(3) + '.mat'\n",
    "    # print(f\"filled_data_file: {filled_data_file}\")\n",
    "\n",
    "    h5py_works = True\n",
    "    # debug_print(\"made it to layerize_h5py\", \"passed the filled_data_file variable to h5py.File\",\n",
    "    # \"\\n\", \"filled_data_file: \", filled_data_file, \"\\n\")\n",
    "    try:\n",
    "        f = h5py.File(filled_data_file, 'r')\n",
    "        debug_print(BRIGHT_GREEN, f\"Opened {filled_data_file} with h5py\")\n",
    "    except:\n",
    "        # debug_print(RED, f\"Error: could not open {filled_data_file} with h5py, ignoring this file\")\n",
    "        # return True\n",
    "        # print(f\"Error: could not open {filled_data_file} with h5py, attempting with sio.loadmat\")\n",
    "        try:\n",
    "            f = sio.loadmat(filled_data_file)\n",
    "            h5py_works = False\n",
    "        except:\n",
    "            debug_print(RED, f\"Error: could not open {filled_data_file} with sio.loadmat, exiting\")\n",
    "            # sys.exit(1)\n",
    "            return True      \n",
    "\n",
    "\n",
    "    # f = h5py.File(data_file + str(3).zfill(3) + '.mat', 'r')  #\n",
    "    # print the contents of the 'param' key\n",
    "    # print(f\"f['param']: {f['param']}\")\n",
    "    # print(f\"f['param'].keys(): {list(f['param'].keys())}\")\n",
    "\n",
    "    filename = data_file + '001.mat'  # fill the data which doesn't change per point\n",
    "    # param = []  # will need some debugging and we probably don't care about it\n",
    "    layer1_quality = []\n",
    "    layer2_quality = []\n",
    "    layer1_type = []\n",
    "    layer2_type = []\n",
    "\n",
    "    layer1_quality.append(f['quality'][0])\n",
    "    layer2_quality.append(f['quality'][1])\n",
    "    layer1_type.append(f['type'][0])\n",
    "    layer2_type.append(f['type'][1])\n",
    "\n",
    "    # initialize the data that will be loaded from the files\n",
    "    gps_time = []\n",
    "    layer1_id = []\n",
    "    layer2_id = []\n",
    "    lat = []\n",
    "    lon = []\n",
    "    layer1_twtt = []\n",
    "    layer2_twtt = []\n",
    "\n",
    "    # print(\"\\n\\nlayerize_h5py debug:\")\n",
    "    # print(f\"f['lat'][:]: {f['lat'][:]}\")\n",
    "    # print(f\"f['lat'][:]: {f['lat'][:].flatten()[0]}\")\n",
    "\n",
    "    # print(f\"f['lat'][:][0]: {f['lat'][:][0]}\")\n",
    "    # print(f\"layer1 lon: {f['lon'][:]}\")\n",
    "\n",
    "    # print(\"\\n\\n\")\n",
    "    already_printed = False\n",
    "    # Load data from each file\n",
    "    if h5py_works:\n",
    "        for i in range(int(startframe), int(endframe) + 1):\n",
    "            filename = data_file + str(i).zfill(3) + '.mat'\n",
    "            # if h5py_works:\n",
    "            with h5py.File(filename, 'r') as f:\n",
    "                gps_time.append(f['gps_time'][:])\n",
    "                layer1_id.append(f['id'][0])\n",
    "                layer2_id.append(f['id'][1])\n",
    "                lat.append(f['lat'][:].flatten())\n",
    "                lon.append(f['lon'][:].flatten())\n",
    "                # param.append(f['param'])\n",
    "                # layer1_quality.append(f['quality'][0])\n",
    "                # layer2_quality.append(f['quality'][1])\n",
    "\n",
    "                # Extract twtt columns directly into layer1_twtt and layer2_twtt\n",
    "                twtt_data = f['twtt'][:]\n",
    "                layer1_twtt.extend(twtt_data[:, 0])\n",
    "                layer2_twtt.extend(twtt_data[:, 1])\n",
    "\n",
    "        # Convert lists to numpy arrays\n",
    "        # debug_print(f\"gps_time: {gps_time}\")\n",
    "\n",
    "        gps_time = np.concatenate(gps_time)\n",
    "        lat = np.concatenate(lat)\n",
    "        lon = np.concatenate(lon)\n",
    "        # param = np.concatenate(param)\n",
    "        layer1_twtt = np.array(layer1_twtt)\n",
    "        layer2_twtt = np.array(layer2_twtt)\n",
    "\n",
    "        # Convert lists to numpy arrays\n",
    "        layer1_id = np.array(layer1_id)\n",
    "        layer2_id = np.array(layer2_id)\n",
    "        layer1_quality = np.array(layer1_quality)\n",
    "        layer2_quality = np.array(layer2_quality)\n",
    "        layer1_type = np.array(layer1_type)\n",
    "        layer2_type = np.array(layer2_type)\n",
    "\n",
    "    else:\n",
    "        unflat_lat = []\n",
    "        unflat_lon = []\n",
    "        for i in range(int(startframe), int(endframe) + 1):\n",
    "            filename = data_file + str(i).zfill(3) + '.mat'\n",
    "\n",
    "            f = sio.loadmat(filename)\n",
    "            # print the keys in f\n",
    "            if already_printed:\n",
    "                print(f.keys())\n",
    "                already_printed = True\n",
    "            for time in f['gps_time']:\n",
    "                # debug_print(f\"time: {time}\")\n",
    "                gps_time.extend(time)\n",
    "\n",
    "            # gps_time.append(f['gps_time'][0])\n",
    "            layer1_id.append(f['id'][0][0])\n",
    "            # for id in f['id']:d\n",
    "            # layer1_id.append(id[0][0])\n",
    "            # layer2_id.append(id[0][1])\n",
    "            layer2_id.append(f['id'][0][1])\n",
    "\n",
    "            unflat_lat.append((f['lat'][0]))\n",
    "            unflat_lon.append(f['lon'][0])\n",
    "            # print(f\"unflat_lat: {f['lat'][0]}\")  # array does not have a second index, just redundant brackets\n",
    "\n",
    "            # Extract twtt columns directly into layer1_twtt and layer2_twtt\n",
    "            twtt_data = f['twtt']\n",
    "            debug_print(f\"len(twtt_data): {len(twtt_data)}\")\n",
    "            layer1_twtt.extend(twtt_data[:, 0])\n",
    "            layer2_twtt.extend(twtt_data[:, 1])\n",
    "        # debug_print(RED, f\"gps_time: {gps_time}\")\n",
    "\n",
    "        gps_time = np.array(gps_time)\n",
    "        # debug_print(f\"gps_time: {gps_time}\")\n",
    "\n",
    "        # Flattening lat and lon arrays into single lists\n",
    "        my_new_lat = []\n",
    "        my_new_lon = []\n",
    "        for arr in unflat_lat:\n",
    "            lat.extend(arr)  # This will append the contents of each array into the list\n",
    "        for arr in unflat_lon:\n",
    "            lon.extend(arr)\n",
    "\n",
    "    # create a layer object\n",
    "    layer1 = Layer(layer1_name, gps_time, layer1_id, lat, lon, layer1_quality, layer1_twtt, layer1_type)\n",
    "    layer2 = Layer(layer2_name, gps_time, layer2_id, lat, lon, layer2_quality, layer2_twtt, layer2_type)\n",
    "\n",
    "    import time\n",
    "    corrected = False\n",
    "    if convert_xy:\n",
    "        start = time.time()\n",
    "        # Convert lat-lon to x-y\n",
    "        print(\"Converting lat-lon to x-y...\")\n",
    "        for i in range(len(lat)):\n",
    "            if i % 100 == 0:\n",
    "                progress_bar(i, len(lat), start)\n",
    "            x1, y1 = latlon_to_xy(lat[i], lon[i])\n",
    "            layer1.x.append(x1)\n",
    "            layer1.y.append(y1)\n",
    "            layer2.x.append(x1)\n",
    "            layer2.y.append(y1)\n",
    "\n",
    "            twtt1 = layer1.twtt - layer1.twtt\n",
    "            twtt2 = layer1.twtt - layer2.twtt\n",
    "            layer1.twtt_corrected = twtt1\n",
    "            layer2.twtt_corrected = twtt2\n",
    "\n",
    "            depth1 = twtt_to_depth(twtt1)\n",
    "            depth2 = twtt_to_depth(twtt2)\n",
    "            layer1.depth = depth1\n",
    "            layer2.depth = depth2\n",
    "\n",
    "        layer1.xy_exists = True\n",
    "        layer2.xy_exists = True\n",
    "\n",
    "    # append the layer to the layers list\n",
    "    layers.append(layer1)\n",
    "    layers.append(layer2)\n",
    "    # print(f\"layer1: {layer1.layer_name} number of points: {layer1.twtt.shape[0]}\")\n",
    "    # print(f\"layer2: {layer2.layer_name} number of points: {layer2.twtt.shape[0]}\")\n",
    "\n",
    "    if not corrected:\n",
    "        for layer in layers:\n",
    "            if h5py_works:\n",
    "                corrected_twtt = layer.twtt - layers[0].twtt  # normalize against the surface layer\n",
    "                # corrected_twtt = layer.twtt\n",
    "                layer.twtt_corrected = corrected_twtt\n",
    "            else:\n",
    "                corrected_twtt = np.array(layer.twtt) - np.array(layers[0].twtt)\n",
    "                layer.twtt_corrected = corrected_twtt\n",
    "\n",
    "    return layers\n"
   ],
   "id": "a98f26ad626d9062",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T21:14:41.827737Z",
     "start_time": "2024-08-12T21:14:41.257379Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# read in the layers from the layer files and save them to a pickle file\n",
    "dir = f\"C:\\\\Users\\\\{username}\\\\Documents\\\\cresis\\\\rds\\\\{season}\\\\CSARP_layer\"\n",
    "print(f\"dir: {dir}\")\n",
    "folders = os.listdir(dir)\n",
    "# print(f\"folders: {folders}\")\n",
    "\n",
    "for folder in folders:\n",
    "    flight = folder\n",
    "    file_name = f\"C:\\\\Users\\\\{username}\\\\Desktop\\\\cresis_project\\\\pickle_jar\\\\layer_export_\" + folder + \".pickle\"\n",
    "    debug_print(f\"file_name: {file_name}\")\n",
    "    if not os.path.isfile(file_name):  # if the file does not exist\n",
    "        print(f\"File {file_name} does not exist. Making it...\")\n",
    "        mat_pickler_h5py(season, flight, testing_mode=testing)  # make it\n",
    "        layers = read_layers(file_name)  # read in the layers from the pickle file\n",
    "        print(f\"File {file_name} created.\")\n",
    "    else:\n",
    "        print(f\"I allegedly found the file {file_name}.\")\n",
    "        layers = read_layers(file_name)  # read in the layers from the pickle file\n",
    "        print(f\"File {file_name} loaded.\")"
   ],
   "id": "f72cdc9d4c3c9c5f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dir: C:\\Users\\moser\\Documents\\cresis\\rds\\2022_Antarctica_BaslerMKB\\CSARP_layer\n",
      "\u001B[93mDEBUG: file_name: C:\\Users\\moser\\Desktop\\cresis_project\\pickle_jar\\layer_export_20221229_01.pickle\u001B[0m\n",
      "I allegedly found the file C:\\Users\\moser\\Desktop\\cresis_project\\pickle_jar\\layer_export_20221229_01.pickle.\n",
      "Reading pickle file...\n",
      "--------------------\n",
      "Surface\n",
      "Bottom\n",
      "--------------------\n",
      "\n",
      "File C:\\Users\\moser\\Desktop\\cresis_project\\pickle_jar\\layer_export_20221229_01.pickle loaded.\n",
      "\u001B[93mDEBUG: file_name: C:\\Users\\moser\\Desktop\\cresis_project\\pickle_jar\\layer_export_20230106_01.pickle\u001B[0m\n",
      "I allegedly found the file C:\\Users\\moser\\Desktop\\cresis_project\\pickle_jar\\layer_export_20230106_01.pickle.\n",
      "Reading pickle file...\n",
      "--------------------\n",
      "Surface\n",
      "Bottom\n",
      "--------------------\n",
      "\n",
      "File C:\\Users\\moser\\Desktop\\cresis_project\\pickle_jar\\layer_export_20230106_01.pickle loaded.\n",
      "\u001B[93mDEBUG: file_name: C:\\Users\\moser\\Desktop\\cresis_project\\pickle_jar\\layer_export_20230107_01.pickle\u001B[0m\n",
      "I allegedly found the file C:\\Users\\moser\\Desktop\\cresis_project\\pickle_jar\\layer_export_20230107_01.pickle.\n",
      "Reading pickle file...\n",
      "--------------------\n",
      "Surface\n",
      "Bottom\n",
      "--------------------\n",
      "\n",
      "File C:\\Users\\moser\\Desktop\\cresis_project\\pickle_jar\\layer_export_20230107_01.pickle loaded.\n",
      "\u001B[93mDEBUG: file_name: C:\\Users\\moser\\Desktop\\cresis_project\\pickle_jar\\layer_export_20230107_02.pickle\u001B[0m\n",
      "I allegedly found the file C:\\Users\\moser\\Desktop\\cresis_project\\pickle_jar\\layer_export_20230107_02.pickle.\n",
      "Reading pickle file...\n",
      "--------------------\n",
      "Surface\n",
      "Bottom\n",
      "--------------------\n",
      "\n",
      "File C:\\Users\\moser\\Desktop\\cresis_project\\pickle_jar\\layer_export_20230107_02.pickle loaded.\n",
      "\u001B[93mDEBUG: file_name: C:\\Users\\moser\\Desktop\\cresis_project\\pickle_jar\\layer_export_20230108_01.pickle\u001B[0m\n",
      "I allegedly found the file C:\\Users\\moser\\Desktop\\cresis_project\\pickle_jar\\layer_export_20230108_01.pickle.\n",
      "Reading pickle file...\n",
      "--------------------\n",
      "Surface\n",
      "Bottom\n",
      "--------------------\n",
      "\n",
      "File C:\\Users\\moser\\Desktop\\cresis_project\\pickle_jar\\layer_export_20230108_01.pickle loaded.\n",
      "\u001B[93mDEBUG: file_name: C:\\Users\\moser\\Desktop\\cresis_project\\pickle_jar\\layer_export_20230109_01.pickle\u001B[0m\n",
      "I allegedly found the file C:\\Users\\moser\\Desktop\\cresis_project\\pickle_jar\\layer_export_20230109_01.pickle.\n",
      "Reading pickle file...\n",
      "--------------------\n",
      "Surface\n",
      "Bottom\n",
      "--------------------\n",
      "\n",
      "File C:\\Users\\moser\\Desktop\\cresis_project\\pickle_jar\\layer_export_20230109_01.pickle loaded.\n",
      "\u001B[93mDEBUG: file_name: C:\\Users\\moser\\Desktop\\cresis_project\\pickle_jar\\layer_export_20230110_01.pickle\u001B[0m\n",
      "I allegedly found the file C:\\Users\\moser\\Desktop\\cresis_project\\pickle_jar\\layer_export_20230110_01.pickle.\n",
      "Reading pickle file...\n",
      "--------------------\n",
      "Surface\n",
      "Bottom\n",
      "--------------------\n",
      "\n",
      "File C:\\Users\\moser\\Desktop\\cresis_project\\pickle_jar\\layer_export_20230110_01.pickle loaded.\n",
      "\u001B[93mDEBUG: file_name: C:\\Users\\moser\\Desktop\\cresis_project\\pickle_jar\\layer_export_20230112_01.pickle\u001B[0m\n",
      "I allegedly found the file C:\\Users\\moser\\Desktop\\cresis_project\\pickle_jar\\layer_export_20230112_01.pickle.\n",
      "Reading pickle file...\n",
      "--------------------\n",
      "Surface\n",
      "Bottom\n",
      "--------------------\n",
      "\n",
      "File C:\\Users\\moser\\Desktop\\cresis_project\\pickle_jar\\layer_export_20230112_01.pickle loaded.\n",
      "\u001B[93mDEBUG: file_name: C:\\Users\\moser\\Desktop\\cresis_project\\pickle_jar\\layer_export_20230113_01.pickle\u001B[0m\n",
      "I allegedly found the file C:\\Users\\moser\\Desktop\\cresis_project\\pickle_jar\\layer_export_20230113_01.pickle.\n",
      "Reading pickle file...\n",
      "--------------------\n",
      "Surface\n",
      "Bottom\n",
      "--------------------\n",
      "\n",
      "File C:\\Users\\moser\\Desktop\\cresis_project\\pickle_jar\\layer_export_20230113_01.pickle loaded.\n",
      "\u001B[93mDEBUG: file_name: C:\\Users\\moser\\Desktop\\cresis_project\\pickle_jar\\layer_export_20230116_01.pickle\u001B[0m\n",
      "I allegedly found the file C:\\Users\\moser\\Desktop\\cresis_project\\pickle_jar\\layer_export_20230116_01.pickle.\n",
      "Reading pickle file...\n",
      "--------------------\n",
      "Surface\n",
      "Bottom\n",
      "--------------------\n",
      "\n",
      "File C:\\Users\\moser\\Desktop\\cresis_project\\pickle_jar\\layer_export_20230116_01.pickle loaded.\n",
      "\u001B[93mDEBUG: file_name: C:\\Users\\moser\\Desktop\\cresis_project\\pickle_jar\\layer_export_20230117_01.pickle\u001B[0m\n",
      "I allegedly found the file C:\\Users\\moser\\Desktop\\cresis_project\\pickle_jar\\layer_export_20230117_01.pickle.\n",
      "Reading pickle file...\n",
      "--------------------\n",
      "Surface\n",
      "Bottom\n",
      "--------------------\n",
      "\n",
      "File C:\\Users\\moser\\Desktop\\cresis_project\\pickle_jar\\layer_export_20230117_01.pickle loaded.\n",
      "\u001B[93mDEBUG: file_name: C:\\Users\\moser\\Desktop\\cresis_project\\pickle_jar\\layer_export_20230120_01.pickle\u001B[0m\n",
      "I allegedly found the file C:\\Users\\moser\\Desktop\\cresis_project\\pickle_jar\\layer_export_20230120_01.pickle.\n",
      "Reading pickle file...\n",
      "--------------------\n",
      "Surface\n",
      "Bottom\n",
      "--------------------\n",
      "\n",
      "File C:\\Users\\moser\\Desktop\\cresis_project\\pickle_jar\\layer_export_20230120_01.pickle loaded.\n",
      "\u001B[93mDEBUG: file_name: C:\\Users\\moser\\Desktop\\cresis_project\\pickle_jar\\layer_export_20230120_02.pickle\u001B[0m\n",
      "I allegedly found the file C:\\Users\\moser\\Desktop\\cresis_project\\pickle_jar\\layer_export_20230120_02.pickle.\n",
      "Reading pickle file...\n",
      "--------------------\n",
      "Surface\n",
      "Bottom\n",
      "--------------------\n",
      "\n",
      "File C:\\Users\\moser\\Desktop\\cresis_project\\pickle_jar\\layer_export_20230120_02.pickle loaded.\n",
      "\u001B[93mDEBUG: file_name: C:\\Users\\moser\\Desktop\\cresis_project\\pickle_jar\\layer_export_20230124_01.pickle\u001B[0m\n",
      "I allegedly found the file C:\\Users\\moser\\Desktop\\cresis_project\\pickle_jar\\layer_export_20230124_01.pickle.\n",
      "Reading pickle file...\n",
      "--------------------\n",
      "Surface\n",
      "Bottom\n",
      "--------------------\n",
      "\n",
      "File C:\\Users\\moser\\Desktop\\cresis_project\\pickle_jar\\layer_export_20230124_01.pickle loaded.\n",
      "\u001B[93mDEBUG: file_name: C:\\Users\\moser\\Desktop\\cresis_project\\pickle_jar\\layer_export_20230124_02.pickle\u001B[0m\n",
      "I allegedly found the file C:\\Users\\moser\\Desktop\\cresis_project\\pickle_jar\\layer_export_20230124_02.pickle.\n",
      "Reading pickle file...\n",
      "--------------------\n",
      "Surface\n",
      "Bottom\n",
      "--------------------\n",
      "\n",
      "File C:\\Users\\moser\\Desktop\\cresis_project\\pickle_jar\\layer_export_20230124_02.pickle loaded.\n",
      "\u001B[93mDEBUG: file_name: C:\\Users\\moser\\Desktop\\cresis_project\\pickle_jar\\layer_export_20230125_01.pickle\u001B[0m\n",
      "I allegedly found the file C:\\Users\\moser\\Desktop\\cresis_project\\pickle_jar\\layer_export_20230125_01.pickle.\n",
      "Reading pickle file...\n",
      "--------------------\n",
      "Surface\n",
      "Bottom\n",
      "--------------------\n",
      "\n",
      "File C:\\Users\\moser\\Desktop\\cresis_project\\pickle_jar\\layer_export_20230125_01.pickle loaded.\n",
      "\u001B[93mDEBUG: file_name: C:\\Users\\moser\\Desktop\\cresis_project\\pickle_jar\\layer_export_20230126_01.pickle\u001B[0m\n",
      "I allegedly found the file C:\\Users\\moser\\Desktop\\cresis_project\\pickle_jar\\layer_export_20230126_01.pickle.\n",
      "Reading pickle file...\n",
      "--------------------\n",
      "Surface\n",
      "Bottom\n",
      "--------------------\n",
      "\n",
      "File C:\\Users\\moser\\Desktop\\cresis_project\\pickle_jar\\layer_export_20230126_01.pickle loaded.\n",
      "\u001B[93mDEBUG: file_name: C:\\Users\\moser\\Desktop\\cresis_project\\pickle_jar\\layer_export_20230127_01.pickle\u001B[0m\n",
      "I allegedly found the file C:\\Users\\moser\\Desktop\\cresis_project\\pickle_jar\\layer_export_20230127_01.pickle.\n",
      "Reading pickle file...\n",
      "--------------------\n",
      "Surface\n",
      "Bottom\n",
      "--------------------\n",
      "\n",
      "File C:\\Users\\moser\\Desktop\\cresis_project\\pickle_jar\\layer_export_20230127_01.pickle loaded.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T21:15:49.323482Z",
     "start_time": "2024-08-12T21:15:49.316605Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# def append_layers(first_flight, second_flight):\n",
    "#     \"\"\"\n",
    "#     :param first_flight: a layer object\n",
    "#     :param second_flight: a layer object\n",
    "#     :return: a combined layer object\n",
    "#     \"\"\"\n",
    "#     # assumes that the flights will both have a bottom and surface layer and no other relevant layers\n",
    "# \n",
    "#     # print(BRIGHT_MAGENTA, section_break, RESET)\n",
    "#     print(\"Appending layers...\")\n",
    "#     # append layers2 to layers1\n",
    "#     # layers1 and layers2 have lists of layers. append each list with a given name to the other list with the same name if it exists. if it does not exist, throw an error\n",
    "#     \n",
    "#     # print(f\"length of first_flight.lat: {len(first_flight[0].lat)}\")\n",
    "#     # print(f\"length of second_flight.lat: {len(second_flight[0].lat)}\")\n",
    "#     # print(f\"sum of lengths: {len(first_flight[0].lat) + len(second_flight[0].lat)}\")\n",
    "# \n",
    "#     # first_flight = layers1\n",
    "#     layer_names = [layer.layer_name for layer in first_flight]\n",
    "#     elevation = np.array([layer.elevation for layer in first_flight])\n",
    "#     gps_time = np.array([layer.gps_time for layer in first_flight])\n",
    "#     id = np.array([layer.id for layer in first_flight])\n",
    "#     lat = first_flight[0].lat\n",
    "#     lon = first_flight[0].lon\n",
    "#     param = np.array([layer.param for layer in first_flight])\n",
    "#     quality = np.array([layer.quality for layer in first_flight])\n",
    "#     type = np.array([layer.type for layer in first_flight])\n",
    "#     twtt = np.array([layer.twtt for layer in first_flight])\n",
    "#     \n",
    "#     # append the second flight\n",
    "#     # if the layer names are the same, append the second flight to the first flight\n",
    "#     for layer in second_flight:\n",
    "#         if layer.layer_name not in layer_names:\n",
    "#             raise ValueError(f\"Layer {layer.layer_name} not found in first flight.\")\n",
    "# \n",
    "#     elevation = np.append(elevation, [layer.elevation for layer in second_flight])\n",
    "#     gps_time = np.append(gps_time, [layer.gps_time for layer in second_flight])\n",
    "#     # id = np.append(id, [layer.id for layer in second_flight])\n",
    "#     \n",
    "#     lat = np.append(lat, second_flight[0].lat)\n",
    "#     lon = np.append(lon, second_flight[0].lon)\n",
    "#     \n",
    "#     param = np.append(param, [layer.param for layer in second_flight])\n",
    "#     quality = np.append(quality, [layer.quality for layer in second_flight])\n",
    "#     type = np.append(type, [layer.type for layer in second_flight])\n",
    "#     twtt = np.append(twtt, [layer.twtt for layer in second_flight])\n",
    "#     \n",
    "#     # print(f\"length of id: {len(id)}\")\n",
    "#     # print(f\"length of lat: {len(lat)}\")\n",
    "#     # print(section_break)\n",
    "#     \n",
    "#     print(BRIGHT_MAGENTA, section_break, RESET, \"\\n\")\n",
    "#     # return first_flight\n",
    "#     \n",
    "#     surface_layer = Layer(layer_names[0], gps_time, id, lat, lon, quality, type[0], twtt[0])\n",
    "#     bottom_layer = Layer(layer_names[1], gps_time, id, lat, lon, quality, type[1], twtt[1])\n",
    "#     \n",
    "#     combined_flights = [surface_layer, bottom_layer]\n",
    "#     \n",
    "#     return combined_flights\n",
    "\n",
    "\n",
    "def append_layers(first_flight, second_flight):\n",
    "    \"\"\"\n",
    "    :param first_flight: a layer object\n",
    "    :param second_flight: a layer object\n",
    "    :return: a combined layer object\n",
    "    \"\"\"\n",
    "    # assumes that the flights will both have a bottom and surface layer and no other relevant layers\n",
    "\n",
    "    # print(BRIGHT_MAGENTA, section_break, RESET)\n",
    "    print(\"Appending layers...\")\n",
    "    # append layers2 to layers1\n",
    "    # layers1 and layers2 have lists of layers. append each list with a given name to the other list with the same name if it exists. if it does not exist, throw an error\n",
    "    \n",
    "    # print(f\"length of first_flight.lat: {len(first_flight[0].lat)}\")\n",
    "    # print(f\"length of second_flight.lat: {len(second_flight[0].lat)}\")\n",
    "    # print(f\"sum of lengths: {len(first_flight[0].lat) + len(second_flight[0].lat)}\")\n",
    "\n",
    "    # first_flight = layers1\n",
    "    surface_1 = first_flight[0]\n",
    "    bottom_1 = first_flight[1]\n",
    "    \n",
    "    print(f\"layer_name0: {surface_1.layer_name}\")\n",
    "    print(f\"layer_name1: {bottom_1.layer_name}\")\n",
    "    \n",
    "    layer_names = [surface_1.layer_name, bottom_1.layer_name]\n",
    "    elevation = np.array(surface_1.elevation)\n",
    "    gps_time = np.array(surface_1.gps_time)\n",
    "    # id = np.array([layer.id for layer in first_flight])\n",
    "    id = surface_1.id\n",
    "    lat = surface_1.lat\n",
    "    lon = surface_1.lon\n",
    "    param = np.array([layer.param for layer in first_flight]) # probably None\n",
    "    # quality = np.array([layer.quality for layer in first_flight])\n",
    "    quality = surface_1.quality[0]\n",
    "    type = surface_1.type\n",
    "    twtt0 = np.array(surface_1.twtt)\n",
    "    twtt1 = np.array(bottom_1.twtt)\n",
    "    \n",
    "    # append the second flight\n",
    "    # if the layer names are the same, append the second flight to the first flight\n",
    "    for layer in second_flight:\n",
    "        if layer.layer_name not in layer_names:\n",
    "            raise ValueError(f\"Layer {layer.layer_name} not found in first flight.\")\n",
    "\n",
    "    surface_2 = second_flight[0]\n",
    "    bottom_2 = second_flight[1]\n",
    "    \n",
    "    elevation = np.append(elevation, surface_2.elevation)\n",
    "    # gps_time = np.append(gps_time, [layer.gps_time for layer in second_flight])\n",
    "    gps_time = np.append(gps_time, surface_2.gps_time)\n",
    "    # id = np.append(id, [layer.id for layer in second_flight])\n",
    "    \n",
    "    lat = np.append(lat, surface_2.lat)\n",
    "    lon = np.append(lon, surface_2.lon)\n",
    "    \n",
    "    param = np.append(param, [layer.param for layer in second_flight])\n",
    "    quality = np.append(quality, [layer.quality for layer in second_flight])\n",
    "    type = np.append(type, [layer.type for layer in second_flight])\n",
    "    # twtt = np.append(twtt, [layer.twtt for layer in second_flight])\n",
    "    twtt0 = np.append(twtt0, surface_2.twtt)\n",
    "    twtt1 = np.append(twtt1, bottom_2.twtt)\n",
    "    \n",
    "    # print(f\"length of id: {len(id)}\")\n",
    "    # print(f\"length of lat: {len(lat)}\")\n",
    "    # print(section_break)\n",
    "    \n",
    "    print(BRIGHT_MAGENTA, section_break, RESET, \"\\n\")\n",
    "    # return first_flight\n",
    "    \n",
    "    surface_layer = Layer(layer_names[0], gps_time, id, lat, lon, quality, twtt0, type[0])\n",
    "    bottom_layer = Layer(layer_names[1], gps_time, id, lat, lon, quality, twtt1, type[1])\n",
    "    \n",
    "    combined_flights = [surface_layer, bottom_layer]\n",
    "    \n",
    "    return combined_flights"
   ],
   "id": "421aa0ec22adc55a",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T21:14:41.841696Z",
     "start_time": "2024-08-12T21:14:41.839166Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "8ac1ecb87b70bb97",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T21:20:42.434087Z",
     "start_time": "2024-08-12T21:20:41.533360Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dir = \"C:\\\\Users\\\\moser\\\\Documents\\\\cresis\\\\rds\\\\\" + season + \"\\\\CSARP_layer\"\n",
    "# print(f\"dir: {dir}\")\n",
    "folders = os.listdir(dir)\n",
    "\n",
    "layers = read_layers(\"C:\\\\Users\\\\moser\\\\Desktop\\\\cresis_project\\\\pickle_jar\\\\layer_export_\" + folders[0] + \".pickle\", quiet=True)\n",
    "# print(f\"len(layers): {len(layers)}\")\n",
    "\n",
    "tot_len = 0\n",
    "\n",
    "for folder in folders[0:3]:\n",
    "    if folder == folders[0]:\n",
    "        tot_len += len(layers[0].lat)\n",
    "        continue\n",
    "    print(BRIGHT_MAGENTA, section_break, RESET, \"\\nfolder: \", folder)\n",
    "    borked = mat_pickler_h5py(season, folder, testing_mode=testing)  # make it\n",
    "    if borked:\n",
    "        debug_print(BRIGHT_RED, f\"Folder {folder} is borked. Skipping it.\")\n",
    "        borked = False\n",
    "        continue        \n",
    "    else:\n",
    "        # print(f\"folder: {folder}\")\n",
    "        layer = read_layers(\"C:\\\\Users\\\\moser\\\\Desktop\\\\cresis_project\\\\pickle_jar\\\\layer_export_\" + folder + \".pickle\", quiet=True)\n",
    "        layer_len = len(layer[0].lat)\n",
    "        # print(f\"len(layer): {layer_len}\")\n",
    "        tot_len += layer_len\n",
    "    layers = append_layers(layers, layer)\n",
    "# print(f\"total length: {tot_len}\")\n",
    "\n",
    "# print the length of each member of the layers[0] object\n",
    "print(f\"length of name array: {len(layers[0].layer_name)}\")\n",
    "# print(f\"length of elevation array: {len(layers[0].elevation)}\")\n",
    "print(f\"length of gps_time array: {len(layers[0].gps_time)}\")\n",
    "print(f\"length of id array: {len(layers[0].id)}\")\n",
    "print(f\"length of lat array: {len(layers[0].lat)}\")\n",
    "print(f\"length of lon array: {len(layers[0].lon)}\")\n",
    "# print(f\"length of param array: {len(layers[0].param)}\")\n",
    "print(f\"length of quality array: {len(layers[0].quality)}\")\n",
    "# print(layers[0].twtt)\n",
    "print(f\"length of twtt array: {len(layers[0].twtt)}\")\n",
    "# print(f\"length of twtt_corrected array: {len(layers[0].twtt_corrected)}\")\n",
    "print(f\"type array: {layers[0].type}\")\n",
    "print(f\"length of x array: {len(layers[0].x)}\")\n",
    "print(f\"length of y array: {len(layers[0].y)}\")\n",
    "# print(f\"length of xy_exists array: {len(layers[0].xy_exists)}\")\n",
    "print(f\"length of depth array: {len(layers[0].depth)}\")\n"
   ],
   "id": "39ec426a31af3fd4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[95m -------------------- \u001B[0m \n",
      "folder:  20230106_01\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "Appending layers...\n",
      "layer_name0: Surface\n",
      "layer_name1: Bottom\n",
      "\u001B[95m -------------------- \u001B[0m \n",
      "\n",
      "\u001B[95m -------------------- \u001B[0m \n",
      "folder:  20230107_01\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "Appending layers...\n",
      "layer_name0: Surface\n",
      "layer_name1: Bottom\n",
      "\u001B[95m -------------------- \u001B[0m \n",
      "\n",
      "length of name array: 7\n",
      "length of gps_time array: 276487\n",
      "length of id array: 28\n",
      "length of lat array: 276487\n",
      "length of lon array: 276487\n",
      "length of quality array: 6669\n",
      "length of twtt array: 166\n",
      "type array: 2\n",
      "length of x array: 0\n",
      "length of y array: 0\n",
      "length of depth array: 0\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f\"length of lat array: {len(layers[0].lat)}\")\n",
    "print(f\"layers[0].twtt: {layers[0].twtt}\")"
   ],
   "id": "1d85cfc8f94a5061",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "794454b9dcbbfb9e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "a26a3112f6946b07",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "directory = os.getcwd() + \"\\\\pickle_jar\\\\\"\n",
    "# file_name = directory + \"layer_export\" + attributes_file[5:-4] + \".pickle\"\n",
    "file_name = directory + \"layer_export_\" + season + \"_full\" + \".pickle\"\n",
    "pickle.dump(layers, open(file_name, \"wb\"))"
   ],
   "id": "abad3d8efdfa4d88",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "8cba42d3d388f599",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T21:17:53.181761Z",
     "start_time": "2024-08-12T21:17:53.136272Z"
    }
   },
   "cell_type": "code",
   "source": [
    "file_name = f\"C:\\\\Users\\\\{username}\\\\Desktop\\\\cresis_project\\\\pickle_jar\\\\layer_export_\" + folders[0] + \".pickle\"\n",
    "tl = read_layers(file_name)\n",
    "# print(f\"id: {tl[0].id}\")\n",
    "print(f\"param: {tl[0].param}\")\n",
    "# print(f\"quality: {tl[0].quality}\")\n",
    "# print(f\"type: {tl[0].type}\")\n",
    "print(f\"len(twtt): {len(tl[0].twtt)}\")\n",
    "print(f\"len(gpstime): {len(tl[0].gps_time)}\")"
   ],
   "id": "287aefdf50873059",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading pickle file...\n",
      "--------------------\n",
      "Surface\n",
      "Bottom\n",
      "--------------------\n",
      "\n",
      "param: None\n",
      "len(twtt): 56\n",
      "len(gpstime): 92881\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T21:33:36.311330Z",
     "start_time": "2024-08-12T21:33:36.058138Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dir = ('C:\\\\Users\\\\moser\\\\Documents\\\\cresis\\\\rds\\\\' + season + '\\\\CSARP_layer\\\\' + flight + '\\\\')\n",
    "# print(f\"dir: {dir}\")\n",
    "folders = os.listdir(dir)\n",
    "data_file = dir + 'Data_' + flight + '_'\n",
    "attributes_file = dir + 'layer_' + flight\n",
    "# attributes_file = dir + '\\\\layer_' + flight + \"\\\\\"\n",
    "\n",
    "layers = []\n",
    "files = os.listdir(dir)  # list of files in the directory\n",
    "# TODO: shift to differentiating data vs layer based on file name. Data starts with \"Data_\" and layer starts with \"layer_\"\n",
    "startframe = '001'  # the first file number in the directory\n",
    "endframe = str(len(files) - 1).zfill(3)  # the number of files in the directory\n",
    "# debug_print(f\"attribute_file: {attribute_file}\")\n",
    "attribute_mat = h5py.File(attributes_file + '.mat', 'r')\n",
    "# print the keys in the attribute file\n",
    "\n",
    "decimal1 = attribute_mat[np.array([attribute_mat['lyr_name'][0][0]])[0]]\n",
    "decimal1_name = [decimal1[i][0] for i in range(len(decimal1))]  # a list of unicode values of the characters\n",
    "decimal2 = attribute_mat[np.array([attribute_mat['lyr_name'][1][0]])[0]]\n",
    "decimal2_name = [decimal2[i][0] for i in range(len(decimal2))]  # a list of unicode values of the characters\n",
    "name1 = ''.join(chr(i) for i in decimal1_name)  # convert the unicode values to a string\n",
    "name2 = ''.join(chr(i) for i in decimal2_name)  # convert the unicode values to a string\n",
    "\n",
    "layer1_name = name1\n",
    "layer2_name = name2\n",
    "\n",
    "# print(f\"data_file: {data_file}\")\n",
    "filled_data_file = data_file + str(1).zfill(3) + '.mat'\n",
    "# print(f\"filled_data_file: {filled_data_file}\")\n",
    "\n",
    "h5py_works = True\n",
    "# debug_print(\"made it to layerize_h5py\", \"passed the filled_data_file variable to h5py.File\",\n",
    "# \"\\n\", \"filled_data_file: \", filled_data_file, \"\\n\")\n",
    "try:\n",
    "    f = h5py.File(filled_data_file, 'r')\n",
    "    debug_print(BRIGHT_GREEN, f\"Opened {filled_data_file} with h5py\")\n",
    "except:\n",
    "    # debug_print(RED, f\"Error: could not open {filled_data_file} with h5py, ignoring this file\")\n",
    "    # return True\n",
    "    # print(f\"Error: could not open {filled_data_file} with h5py, attempting with sio.loadmat\")\n",
    "    try:\n",
    "        f = sio.loadmat(filled_data_file)\n",
    "        h5py_works = False\n",
    "    except:\n",
    "        debug_print(RED, f\"Error: could not open {filled_data_file} with sio.loadmat, exiting\")\n",
    "        # sys.exit(1)\n",
    "        # return True      \n",
    "\n",
    "\n",
    "# f = h5py.File(data_file + str(3).zfill(3) + '.mat', 'r')  #\n",
    "# print the contents of the 'param' key\n",
    "# print(f\"f['param']: {f['param']}\")\n",
    "# print(f\"f['param'].keys(): {list(f['param'].keys())}\")\n",
    "\n",
    "filename = data_file + '001.mat'  # fill the data which doesn't change per point\n",
    "# param = []  # will need some debugging and we probably don't care about it\n",
    "layer1_quality = []\n",
    "layer2_quality = []\n",
    "layer1_type = []\n",
    "layer2_type = []\n",
    "\n",
    "layer1_quality.append(f['quality'][0])\n",
    "layer2_quality.append(f['quality'][1])\n",
    "layer1_type.append(f['type'][0])\n",
    "layer2_type.append(f['type'][1])\n",
    "\n",
    "# initialize the data that will be loaded from the files\n",
    "gps_time = []\n",
    "layer1_id = []\n",
    "layer2_id = []\n",
    "lat = []\n",
    "lon = []\n",
    "layer1_twtt = []\n",
    "layer2_twtt = []\n",
    "\n",
    "# print(\"\\n\\nlayerize_h5py debug:\")\n",
    "# print(f\"f['lat'][:]: {f['lat'][:]}\")\n",
    "# print(f\"f['lat'][:]: {f['lat'][:].flatten()[0]}\")\n",
    "\n",
    "# print(f\"f['lat'][:][0]: {f['lat'][:][0]}\")\n",
    "# print(f\"layer1 lon: {f['lon'][:]}\")\n",
    "\n",
    "# print(\"\\n\\n\")\n",
    "already_printed = False\n",
    "# Load data from each file\n",
    "if h5py_works:\n",
    "    for i in range(int(startframe), int(endframe) + 1):\n",
    "        filename = data_file + str(i).zfill(3) + '.mat'\n",
    "        # if h5py_works:\n",
    "        with h5py.File(filename, 'r') as f:\n",
    "            gps_time.append(f['gps_time'][:])\n",
    "            layer1_id.append(f['id'][0])\n",
    "            layer2_id.append(f['id'][1])\n",
    "            lat.append(f['lat'][:].flatten())\n",
    "            lon.append(f['lon'][:].flatten())\n",
    "            # param.append(f['param'])\n",
    "            # layer1_quality.append(f['quality'][0])\n",
    "            # layer2_quality.append(f['quality'][1])\n",
    "\n",
    "            # Extract twtt columns directly into layer1_twtt and layer2_twtt\n",
    "            twtt_data = f['twtt'][:]\n",
    "            layer1_twtt.extend(twtt_data[:, 0])\n",
    "            layer2_twtt.extend(twtt_data[:, 1])\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    # debug_print(f\"gps_time: {gps_time}\")\n",
    "\n",
    "    gps_time = np.concatenate(gps_time)\n",
    "    lat = np.concatenate(lat)\n",
    "    lon = np.concatenate(lon)\n",
    "    # param = np.concatenate(param)\n",
    "    layer1_twtt = np.array(layer1_twtt)\n",
    "    layer2_twtt = np.array(layer2_twtt)\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    layer1_id = np.array(layer1_id)\n",
    "    layer2_id = np.array(layer2_id)\n",
    "    layer1_quality = np.array(layer1_quality)\n",
    "    layer2_quality = np.array(layer2_quality)\n",
    "    layer1_type = np.array(layer1_type)\n",
    "    layer2_type = np.array(layer2_type)\n",
    "\n",
    "else:\n",
    "    unflat_lat = []\n",
    "    unflat_lon = []\n",
    "    for i in range(int(startframe), int(endframe) + 1):\n",
    "        filename = data_file + str(i).zfill(3) + '.mat'\n",
    "\n",
    "        f = sio.loadmat(filename)\n",
    "        # print the keys in f\n",
    "        if already_printed:\n",
    "            print(f.keys())\n",
    "            already_printed = True\n",
    "        for time in f['gps_time']:\n",
    "            # debug_print(f\"time: {time}\")\n",
    "            gps_time.extend(time)\n",
    "\n",
    "        # gps_time.append(f['gps_time'][0])\n",
    "        layer1_id.append(f['id'][0][0])\n",
    "        # for id in f['id']:d\n",
    "        # layer1_id.append(id[0][0])\n",
    "        # layer2_id.append(id[0][1])\n",
    "        layer2_id.append(f['id'][0][1])\n",
    "\n",
    "        unflat_lat.append((f['lat'][0]))\n",
    "        unflat_lon.append(f['lon'][0])\n",
    "        # print(f\"unflat_lat: {f['lat'][0]}\")  # array does not have a second index, just redundant brackets\n",
    "\n",
    "        # Extract twtt columns directly into layer1_twtt and layer2_twtt\n",
    "        twtt_data = f['twtt']\n",
    "        debug_print(f\"len(twtt_data): {len(twtt_data)}\")\n",
    "        layer1_twtt.extend(twtt_data[:, 0])\n",
    "        layer2_twtt.extend(twtt_data[:, 1])\n",
    "    # debug_print(RED, f\"gps_time: {gps_time}\")\n",
    "\n",
    "    gps_time = np.array(gps_time)\n",
    "    # debug_print(f\"gps_time: {gps_time}\")\n",
    "\n",
    "    # Flattening lat and lon arrays into single lists\n",
    "    my_new_lat = []\n",
    "    my_new_lon = []\n",
    "    for arr in unflat_lat:\n",
    "        lat.extend(arr)  # This will append the contents of each array into the list\n",
    "    for arr in unflat_lon:\n",
    "        lon.extend(arr)\n",
    "\n",
    "# create a layer object\n",
    "layer1 = Layer(layer1_name, gps_time, layer1_id, lat, lon, layer1_quality, layer1_twtt, layer1_type)\n",
    "layer2 = Layer(layer2_name, gps_time, layer2_id, lat, lon, layer2_quality, layer2_twtt, layer2_type)\n",
    "\n",
    "import time\n",
    "corrected = False\n",
    "if convert_xy:\n",
    "    start = time.time()\n",
    "    # Convert lat-lon to x-y\n",
    "    print(\"Converting lat-lon to x-y...\")\n",
    "    for i in range(len(lat)):\n",
    "        if i % 100 == 0:\n",
    "            progress_bar(i, len(lat), start)\n",
    "        x1, y1 = latlon_to_xy(lat[i], lon[i])\n",
    "        layer1.x.append(x1)\n",
    "        layer1.y.append(y1)\n",
    "        layer2.x.append(x1)\n",
    "        layer2.y.append(y1)\n",
    "\n",
    "        twtt1 = layer1.twtt - layer1.twtt\n",
    "        twtt2 = layer1.twtt - layer2.twtt\n",
    "        layer1.twtt_corrected = twtt1\n",
    "        layer2.twtt_corrected = twtt2\n",
    "\n",
    "        depth1 = twtt_to_depth(twtt1)\n",
    "        depth2 = twtt_to_depth(twtt2)\n",
    "        layer1.depth = depth1\n",
    "        layer2.depth = depth2\n",
    "\n",
    "    layer1.xy_exists = True\n",
    "    layer2.xy_exists = True\n",
    "\n",
    "# append the layer to the layers list\n",
    "layers.append(layer1)\n",
    "layers.append(layer2)\n",
    "# print(f\"layer1: {layer1.layer_name} number of points: {layer1.twtt.shape[0]}\")\n",
    "# print(f\"layer2: {layer2.layer_name} number of points: {layer2.twtt.shape[0]}\")\n",
    "\n",
    "if not corrected:\n",
    "    for layer in layers:\n",
    "        if h5py_works:\n",
    "            corrected_twtt = layer.twtt - layers[0].twtt  # normalize against the surface layer\n",
    "            # corrected_twtt = layer.twtt\n",
    "            layer.twtt_corrected = corrected_twtt\n",
    "        else:\n",
    "            corrected_twtt = np.array(layer.twtt) - np.array(layers[0].twtt)\n",
    "            layer.twtt_corrected = corrected_twtt"
   ],
   "id": "18f9165f919612b9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n",
      "\u001B[93mDEBUG: len(twtt_data): 2\u001B[0m\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'convert_xy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[30], line 173\u001B[0m\n\u001B[0;32m    171\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtime\u001B[39;00m\n\u001B[0;32m    172\u001B[0m corrected \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m--> 173\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mconvert_xy\u001B[49m:\n\u001B[0;32m    174\u001B[0m     start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m    175\u001B[0m     \u001B[38;5;66;03m# Convert lat-lon to x-y\u001B[39;00m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'convert_xy' is not defined"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "79fd2faf3b95238e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
