{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-16T05:03:14.342178Z",
     "start_time": "2024-04-16T05:03:09.977754Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from project_classes import *\n",
    "from functions import *\n",
    "from iceflow_library import *"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rj\\Desktop\\cresis_data_II\\rds\\2016_Antarctica_DC8\\CSARP_layer\\20161024_05\\\n"
     ]
    },
    {
     "data": {
      "text/plain": "['Data_20161024_05_001.mat',\n 'Data_20161024_05_002.mat',\n 'Data_20161024_05_003.mat',\n 'Data_20161024_05_004.mat',\n 'Data_20161024_05_005.mat',\n 'Data_20161024_05_006.mat',\n 'Data_20161024_05_007.mat',\n 'Data_20161024_05_008.mat',\n 'Data_20161024_05_009.mat',\n 'Data_20161024_05_010.mat',\n 'Data_20161024_05_011.mat',\n 'Data_20161024_05_012.mat',\n 'Data_20161024_05_013.mat',\n 'Data_20161024_05_014.mat',\n 'Data_20161024_05_015.mat',\n 'Data_20161024_05_016.mat',\n 'Data_20161024_05_017.mat',\n 'Data_20161024_05_018.mat',\n 'Data_20161024_05_019.mat',\n 'Data_20161024_05_020.mat',\n 'Data_20161024_05_021.mat',\n 'Data_20161024_05_022.mat',\n 'Data_20161024_05_023.mat',\n 'Data_20161024_05_024.mat',\n 'Data_20161024_05_025.mat',\n 'Data_20161024_05_026.mat',\n 'Data_20161024_05_027.mat',\n 'Data_20161024_05_028.mat',\n 'Data_20161024_05_029.mat',\n 'Data_20161024_05_030.mat',\n 'Data_20161024_05_031.mat',\n 'Data_20161024_05_032.mat',\n 'Data_20161024_05_033.mat',\n 'Data_20161024_05_034.mat',\n 'Data_20161024_05_035.mat',\n 'Data_20161024_05_036.mat',\n 'Data_20161024_05_037.mat',\n 'Data_20161024_05_038.mat',\n 'Data_20161024_05_039.mat',\n 'Data_20161024_05_040.mat',\n 'Data_20161024_05_041.mat',\n 'Data_20161024_05_042.mat',\n 'Data_20161024_05_043.mat',\n 'Data_20161024_05_044.mat',\n 'layer_20161024_05.mat']"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dir = 'C:\\\\Users\\\\rj\\\\Documents\\\\cresis\\\\rds\\\\2016_Antarctica_DC8\\\\CSARP_layer\\\\20161024_05\\\\'\n",
    "dir = 'C:\\\\Users\\\\rj\\\\Desktop\\\\cresis_data_II\\\\rds\\\\2016_Antarctica_DC8\\\\CSARP_layer\\\\20161024_05\\\\'\n",
    "\n",
    "print(dir)\n",
    "os.listdir(dir)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T05:03:14.357710Z",
     "start_time": "2024-04-16T05:03:14.344180Z"
    }
   },
   "id": "43c100fb838da459",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "segment_data_file = 'Data_20161024_05_'\n",
    "\n",
    "startframe = '001'\n",
    "# endframe = '015'\n",
    "# endframe = the number of files in the directory\n",
    "files = os.listdir(dir)\n",
    "endframe = str(len(files) - 1).zfill(3)\n",
    "\n",
    "# data_mat = np.array([sio.loadmat(dir + segment_data_file + str(i).zfill(3) + '.mat')\n",
    "#                      for i in range(int(startframe), int(endframe)+1)])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T05:03:14.374069Z",
     "start_time": "2024-04-16T05:03:14.359711Z"
    }
   },
   "id": "18bc8607afa5cd7",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import h5py"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T05:03:14.389725Z",
     "start_time": "2024-04-16T05:03:14.376197Z"
    }
   },
   "id": "53054f2a988fd180",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# use the h5py library to read the data\n",
    "# f = h5py.File(dir + segment_data_file + '001.mat', 'r')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T05:03:14.405096Z",
     "start_time": "2024-04-16T05:03:14.391727Z"
    }
   },
   "id": "49b9da04e544cc91",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "endframe: 044\n",
      "<HDF5 dataset \"elev\": shape (3442, 1), type \"<f8\">\n"
     ]
    }
   ],
   "source": [
    "# get the keys of the file\n",
    "# list(f.keys())\n",
    "flight = '20161024_05'\n",
    "\n",
    "segment_data_file = 'Data_' + flight + '_'\n",
    "layer_attributes_file = 'layer_' + flight + '.mat'\n",
    "\n",
    "startframe = '001'\n",
    "# endframe = the number of files in the directory\n",
    "files = os.listdir(dir)\n",
    "endframe = str(len(files) - 1).zfill(3)\n",
    "print(f\"endframe: {endframe}\")\n",
    "\n",
    "# load an array of mat files\n",
    "# data_mat = []\n",
    "# for i in range(int(startframe), int(endframe)+1):\n",
    "#     f = h5py.File(dir + segment_data_file + str(0).zfill(3) + '.mat', 'r')\n",
    "# f = h5py.File(dir + segment_data_file + '001.mat', 'r')\n",
    "\n",
    "\n",
    "data_mat = []\n",
    "for i in range(int(startframe), int(endframe)+1):\n",
    "    f = h5py.File(dir + segment_data_file + str(i).zfill(3) + '.mat', 'r')\n",
    "    data_mat.append(f)\n",
    "elevation = np.array([])\n",
    "# elevation = np.append(elevation, data_mat['elev'])\n",
    "print((data_mat[43]['elev']))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T05:06:29.503296Z",
     "start_time": "2024-04-16T05:06:29.475320Z"
    }
   },
   "id": "da7d77c1cdb99d75",
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **TODO: 16Apr24**\n",
    "- look at what ChatGPT said, I'm too tired to implement it\n",
    "- **move the list definitions up above the h5py reading call**\n",
    "    - define empty lists\n",
    "    - in the loop, read the file f\n",
    "    - in the loop append each list in f to the relevant list\n",
    "    - in the end you have continuous lists\n",
    "    - then you pickle them or whatever **"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "136a8e116fe77886"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def mat_pickler_h5py(season, flight, testing_mode=False, readout=False, save=True, plot_layer=False):\n",
    "    print(\"Reading data files...\")\n",
    "    print(\"--------------------\")\n",
    "    segment_data_file = 'Data_' + flight + '_'\n",
    "    layer_attributes_file = 'layer_' + flight + '.mat'\n",
    "    \n",
    "    # set the directory, segment data file, layer attributes file, and start and end frames\n",
    "    if testing_mode:\n",
    "        # the dir is the current directory + the test_data folder\n",
    "        dir = os.getcwd() + '\\\\test_data\\\\' + flight + '\\\\'\n",
    "        \n",
    "            # dir = ('test_data') + '\\\\'\n",
    "            # contains all of the actual data such as twtt, lat, lon, etc.\n",
    "            # contains the attributes of the layer such as name, param, etc.\n",
    "    else:\n",
    "        # TODO: refactor this into a try except block, maybe upstream of this function where it is called\n",
    "        dir = ('C:\\\\Users\\\\rj\\\\Documents\\\\cresis\\\\rds\\\\' + season + '\\\\CSARP_layer\\\\' + flight + '\\\\')\n",
    "        #     # leaving because it might actually be good. the below line works for 2018_Antarctica_DC8 at least as it\n",
    "        #     # has the CSARP_layerData folder instead of CSARP_layer.\n",
    "    \n",
    "        # contains all of the actual data such as twtt, lat, lon, etc.\n",
    "        # contains the attributes of the layer such as name, param, etc.\n",
    "        # print(f\"layer_attributes_file: {layer_attributes_file}\")\n",
    "    # contains the attributes of the layer such as name, param, etc.\n",
    "    \n",
    "    startframe = '001'\n",
    "    # endframe = '015'\n",
    "    # endframe = the number of files in the directory\n",
    "    files = os.listdir(dir)\n",
    "    endframe = str(len(files) - 1).zfill(3)\n",
    "    \n",
    "    # load an array of mat files\n",
    "    data_mat = []\n",
    "    for i in range(int(startframe), int(endframe)+1):\n",
    "        f = h5py.File(dir + segment_data_file + str(i).zfill(3) + '.mat', 'r')\n",
    "        data_mat.append(f)\n",
    "\n",
    "    \n",
    "    # print(f\"size of f: {len(f)}\")\n",
    "    # print(f\"size of data_mat: {len(data_mat)}\")\n",
    "    # print(f\"first 3 elev in f: {f['elev'][:3]}\")\n",
    "    # print(f\"first 3 elev in data_mat: {data_mat[0]['elev'][:3]}\")\n",
    "    \n",
    "    attribute_mat = h5py.File(dir + layer_attributes_file, 'r')\n",
    "\n",
    "    layers = layerize_h5py(data_mat, attribute_mat)\n",
    "\n",
    "    if readout:\n",
    "        print(\"--------------------\", end=\"\")\n",
    "        for layer in layers:\n",
    "            print(f\"\\n{layer.layer_name} number of points: {layer.twtt.shape[0]}\")\n",
    "            print(f\"{layer.layer_name} twtt first three: {layer.twtt[:3].tolist()} \")\n",
    "            print(f\"{layer.layer_name} twtt last three: {layer.twtt[-3:].tolist()} \")\n",
    "        print(\"--------------------\\n\")\n",
    "    \n",
    "    if save:\n",
    "        # save layers to a pickle file\n",
    "        # print(\"Saving layers to a pickle file...\")\n",
    "        print(\"--------------------\")\n",
    "        # list current directory\n",
    "        # print(f\"Current directory: {}\")\n",
    "        directory = os.getcwd() + \"\\\\pickle_jar\\\\\"\n",
    "        file_name = directory + \"layer_export\" + layer_attributes_file[5:-4] + \".pickle\"\n",
    "        pickle.dump(layers, open(file_name, \"wb\"))\n",
    "        print(file_name, \" saved in local directory of this python file.\")\n",
    "        print(\"--------------------\\n\")\n",
    "\n",
    "    if plot_layer:\n",
    "        # plot the layers\n",
    "        print(\"Plotting layers...\")\n",
    "        print(\"--------------------\")\n",
    "        # plot the layer depths vs gps time for each layer on the same plot\n",
    "        for layer in layers:\n",
    "            plt.plot(layer.gps_time, layer.twtt, label=layer.layer_name)\n",
    "        plt.xlabel(\"GPS Time\")\n",
    "        plt.ylabel(\"Two Way Travel Time (ns)\")\n",
    "        plt.title(\"Elevation vs GPS Time\")\n",
    "        plt.legend()\n",
    "\n",
    "        plt.show()\n",
    "        print(\"--------------------\\n\")\n",
    "        \n",
    "def layerize_h5py(data_mat, attribute_mat):\n",
    "    # TODO: add a docstring\n",
    "    # TODO: add a portion to pull the layer name out of the attribute file\n",
    "        # afai can tell, that is all the attribute file is used for in the original code\n",
    "    # create an empty list to store the layers\n",
    "    layers = []\n",
    "    \n",
    "    print(f\"attribute_mat keys: {list(attribute_mat.keys())}\")\n",
    "    # print(f\"attribute_mat['lyr_name']: {attribute_mat['lyr_name']}\")\n",
    "\n",
    "    # iterate through the data_mat\n",
    "    # for i in range(2):\n",
    "    #     layer_name = \"layer_\" + str(i)\n",
    "    #     elevation = np.array([])\n",
    "    #     gps_time = np.array([])\n",
    "    #     id = np.array([])\n",
    "    #     lat = np.array([])\n",
    "    #     lon = np.array([])\n",
    "    #     param = np.array([])\n",
    "    #     quality = np.array([])\n",
    "    #     twtt = np.array([])\n",
    "    #     layer_type = np.array([])\n",
    "    #     for data in data_mat:\n",
    "    #         elevation = np.append(elevation, data['elev'])\n",
    "    #         gps_time = np.append(gps_time, data['gps_time'])\n",
    "    #         id = np.append(id, data['id'])\n",
    "    #         lat = np.append(lat, data['lat'])\n",
    "    #         lon = np.append(lon, data['lon'])\n",
    "    #         param = np.append(param, data['param'])\n",
    "    #         quality = np.append(quality, data['quality'])\n",
    "    #         twtt = np.append(twtt, data['twtt'])\n",
    "    #         layer_type = np.append(layer_type, data['type'])\n",
    "    \n",
    "        #     layers.append(Layer(layer_name, gps_time, id, lat, lon, param, quality, twtt, type, elevation))\n",
    "        # print(f\"Layer {i} created.\")\n",
    "    id = 0 # layer id number (0 or 1)\n",
    "\n",
    "    decimal1 = attribute_mat[np.array([attribute_mat['lyr_name'][0][0]])[0]]\n",
    "    decimal_name1 = [decimal1[i][0] for i in range(len(decimal1))]\n",
    "    \n",
    "    decimal2 = attribute_mat[np.array([attribute_mat['lyr_name'][1][0]])[0]]\n",
    "    decimal_name2 = [decimal2[i][0] for i in range(len(decimal2))]\n",
    "    # print(''.join(chr(i) for i in decimal_name1))\n",
    "    layer1_name = ''.join(chr(i) for i in decimal_name1)\n",
    "    layer2_name = ''.join(chr(i) for i in decimal_name2)\n",
    "    print(f\"layer1_name: {layer1_name}\")\n",
    "    print(f\"layer2_name: {layer2_name}\")\n",
    "    elevation = np.array([])\n",
    "    gps_time = np.array([])\n",
    "    layer1_id = np.array([])\n",
    "    layer2_id = np.array([])\n",
    "    lat = np.array([])\n",
    "    lon = np.array([])\n",
    "    param = np.array([])\n",
    "    layer1_quality = np.array([])\n",
    "    layer2_quality = np.array([])\n",
    "    layer1_twtt = np.array([])\n",
    "    layer2_twtt = np.array([])\n",
    "    layer1_type = np.array([])\n",
    "    layer2_type = np.array([])\n",
    "    \n",
    "    # for i in range(len(f['elev'])):\n",
    "    #     # get the data from the data_mat\n",
    "    #     elevation = np.append(elevation, f['elev'][i])\n",
    "    #     gps_time = np.append(gps_time, f['gps_time'][i])\n",
    "    #     layer1_id = np.append(layer1_id, f['id'][0])\n",
    "    #     layer2_id = np.append(layer2_id, f['id'][1])\n",
    "    #     lat = np.append(lat, f['lat'][i])\n",
    "    #     lon = np.append(lon, f['lon'][i])\n",
    "    #     param = np.append(param, f['param'])\n",
    "    #     layer1_quality = np.append(layer1_quality, f['quality'][0])\n",
    "    #     layer2_quality = np.append(layer2_quality, f['quality'][1])\n",
    "    #     layer1_twtt = np.append(layer1_twtt, f['twtt'][0])\n",
    "    #     layer2_twtt = np.append(layer2_twtt, f['twtt'][1])\n",
    "    #     layer1_type = np.append(layer1_type, f['type'][0])\n",
    "    #     layer2_type = np.append(layer2_type, f['type'][1])        \n",
    "    \n",
    "    print(f\"\\n\\ndata_mat['elev'][0]: {data_mat[0]['elev']}\\n\\n\")\n",
    "    length = data_mat[0]['elev'].shape[0]\n",
    "    for i in range(length):\n",
    "        # get the data from the data_mat\n",
    "        elevation = np.append(elevation, data_mat['elev'][i])\n",
    "        gps_time = np.append(gps_time, data_mat['gps_time'][i])\n",
    "        layer1_id = np.append(layer1_id, data_mat['id'][0])\n",
    "        layer2_id = np.append(layer2_id, data_mat['id'][1])\n",
    "        lat = np.append(lat, data_mat['lat'][i])\n",
    "        lon = np.append(lon, data_mat['lon'][i])\n",
    "        param = np.append(param, data_mat['param'])\n",
    "        layer1_quality = np.append(layer1_quality, data_mat['quality'][0])\n",
    "        layer2_quality = np.append(layer2_quality, data_mat['quality'][1])\n",
    "        layer1_twtt = np.append(layer1_twtt, data_mat['twtt'][0])\n",
    "        layer2_twtt = np.append(layer2_twtt, data_mat['twtt'][1])\n",
    "        layer1_type = np.append(layer1_type, data_mat['type'][0])\n",
    "        layer2_type = np.append(layer2_type, data_mat['type'][1])   \n",
    "    \n",
    "    # print the size of gps_time and twtt\n",
    "    print(f\"gps_time size: {gps_time.shape}\")\n",
    "    print(f\"twtt size: {layer1_twtt.shape}\")\n",
    "    \n",
    "    \n",
    "    # create a layer object\n",
    "    layer1 = Layer(layer1_name, gps_time, layer1_id, lat, lon, param, layer1_quality, layer1_twtt, layer1_type, elevation)\n",
    "    \n",
    "    layer2 = Layer(layer2_name, gps_time, layer2_id, lat, lon, param, layer2_quality, layer2_twtt, layer2_type, elevation)\n",
    "    \n",
    "    # append the layer to the layers list\n",
    "    layers.append(layer1)\n",
    "    layers.append(layer2)\n",
    "    print(f\"layer1: {layer1.layer_name} number of points: {layer1.twtt.shape[0]}\")\n",
    "    print(f\"layer2: {layer2.layer_name} number of points: {layer2.twtt.shape[0]}\")\n",
    "    \n",
    "    return layers\n",
    "\n",
    "def helper(mat, input1, index1, index2=None, index3=None):\n",
    "    # decimal2 = attribute_mat[np.array([attribute_mat['lyr_name'][1][0]])[0]]\n",
    "    # decimal_name2 = [decimal2[i][0] for i in range(len(decimal2))]\n",
    "    if index2:\n",
    "        mat_content = mat[np.array(mat[input][index1])[index2]]\n",
    "    if index3:\n",
    "        mat_content = mat[np.array(mat[input1][index1][index2])[index3]]\n",
    "    else:\n",
    "        mat_content = mat[np.array(mat[input1])]\n",
    "    mat_name = [mat_content[i][0] for i in range(len(mat_content))]\n",
    "    return mat_name"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T04:59:11.004235Z",
     "start_time": "2024-04-16T04:59:10.979078Z"
    }
   },
   "id": "f795a01250a2c4ba",
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data files...\n",
      "--------------------\n",
      "size of f: 11\n",
      "size of data_mat: 44\n",
      "first 3 elev in f: [[9981.19338401]\n",
      " [9981.17779532]\n",
      " [9981.16082438]]\n",
      "first 3 elev in data_mat: [[9085.58101288]\n",
      " [9085.58756533]\n",
      " [9085.59432642]]\n",
      "attribute_mat keys: ['#refs#', 'file_type', 'file_version', 'lyr_age', 'lyr_age_source', 'lyr_desc', 'lyr_group_name', 'lyr_id', 'lyr_name', 'lyr_order', 'param']\n",
      "layer1_name: surface\n",
      "layer2_name: bottom\n",
      "\n",
      "\n",
      "data_mat['elev'][0]: <HDF5 dataset \"elev\": shape (3334, 1), type \"<f8\">\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[30], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mmat_pickler_h5py\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m2016_Antarctica_DC8\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m20161024_05\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtesting_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreadout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msave\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mplot_layer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[29], line 55\u001B[0m, in \u001B[0;36mmat_pickler_h5py\u001B[1;34m(season, flight, testing_mode, readout, save, plot_layer)\u001B[0m\n\u001B[0;32m     51\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfirst 3 elev in data_mat: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdata_mat[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124melev\u001B[39m\u001B[38;5;124m'\u001B[39m][:\u001B[38;5;241m3\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     53\u001B[0m attribute_mat \u001B[38;5;241m=\u001B[39m h5py\u001B[38;5;241m.\u001B[39mFile(\u001B[38;5;28mdir\u001B[39m \u001B[38;5;241m+\u001B[39m layer_attributes_file, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m---> 55\u001B[0m layers \u001B[38;5;241m=\u001B[39m \u001B[43mlayerize_h5py\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_mat\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattribute_mat\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     57\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m readout:\n\u001B[0;32m     58\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m--------------------\u001B[39m\u001B[38;5;124m\"\u001B[39m, end\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[29], line 173\u001B[0m, in \u001B[0;36mlayerize_h5py\u001B[1;34m(data_mat, attribute_mat)\u001B[0m\n\u001B[0;32m    170\u001B[0m length \u001B[38;5;241m=\u001B[39m data_mat[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124melev\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    171\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(length):\n\u001B[0;32m    172\u001B[0m     \u001B[38;5;66;03m# get the data from the data_mat\u001B[39;00m\n\u001B[1;32m--> 173\u001B[0m     elevation \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mappend(elevation, \u001B[43mdata_mat\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43melev\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m[i])\n\u001B[0;32m    174\u001B[0m     gps_time \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mappend(gps_time, data_mat[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgps_time\u001B[39m\u001B[38;5;124m'\u001B[39m][i])\n\u001B[0;32m    175\u001B[0m     layer1_id \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mappend(layer1_id, data_mat[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mid\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;241m0\u001B[39m])\n",
      "\u001B[1;31mTypeError\u001B[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "mat_pickler_h5py('2016_Antarctica_DC8', '20161024_05', testing_mode=False, readout=False, save=True, plot_layer=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T04:59:11.331051Z",
     "start_time": "2024-04-16T04:59:11.150624Z"
    }
   },
   "id": "393d1716d79e50c1",
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "['elev',\n 'file_type',\n 'file_version',\n 'gps_time',\n 'id',\n 'lat',\n 'lon',\n 'param',\n 'quality',\n 'twtt',\n 'type']"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(f.keys())\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T04:54:50.365676Z",
     "start_time": "2024-04-16T04:54:50.349670Z"
    }
   },
   "id": "123886f2a4cd6d0d",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 dataset \"elev\": shape (3334, 1), type \"<f8\">\n",
      "<HDF5 dataset \"file_type\": shape (5, 1), type \"<u2\">\n",
      "<HDF5 dataset \"file_version\": shape (1, 1), type \"<u2\">\n",
      "<HDF5 dataset \"gps_time\": shape (3334, 1), type \"<f8\">\n",
      "<HDF5 dataset \"id\": shape (2, 1), type \"<f8\">\n",
      "<HDF5 dataset \"lat\": shape (3334, 1), type \"<f8\">\n",
      "<HDF5 dataset \"lon\": shape (3334, 1), type \"<f8\">\n",
      "<HDF5 group \"/param\" (6 members)>\n",
      "<HDF5 dataset \"quality\": shape (3334, 2), type \"|u1\">\n",
      "<HDF5 dataset \"twtt\": shape (3334, 2), type \"<f8\">\n",
      "<HDF5 dataset \"type\": shape (3334, 2), type \"|u1\">\n"
     ]
    }
   ],
   "source": [
    "print(f['elev'])\n",
    "print(f['file_type'])\n",
    "print(f['file_version'])\n",
    "print(f['gps_time'])\n",
    "print(f['id'])\n",
    "print(f['lat'])\n",
    "print(f['lon'])\n",
    "print(f['param'])\n",
    "print(f['quality'])\n",
    "print(f['twtt'])\n",
    "print(f['type'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T00:58:45.404251Z",
     "start_time": "2024-04-16T00:58:45.387793Z"
    }
   },
   "id": "f98cf00f0a45f724",
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "flight = '20161024_05'\n",
    "layer_attributes_file = 'layer_' + flight + '.mat'\n",
    "\n",
    "attribute_mat = h5py.File(dir + layer_attributes_file, 'r')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T01:20:01.625865Z",
     "start_time": "2024-04-16T01:20:01.617042Z"
    }
   },
   "id": "4afb0fdaea293570",
   "execution_count": 50
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attribute_mat keys: ['#refs#', 'file_type', 'file_version', 'lyr_age', 'lyr_age_source', 'lyr_desc', 'lyr_group_name', 'lyr_id', 'lyr_name', 'lyr_order', 'param']\n"
     ]
    }
   ],
   "source": [
    "print(f\"attribute_mat keys: {list(attribute_mat.keys())}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T01:20:10.391927Z",
     "start_time": "2024-04-16T01:20:10.384062Z"
    }
   },
   "id": "b562eab538a0e47a",
   "execution_count": 51
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attribute_mat['lyr_name']: [<HDF5 object reference>]\n"
     ]
    }
   ],
   "source": [
    "print(f\"attribute_mat['lyr_name']: {attribute_mat['lyr_name'][1]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T01:20:47.664170Z",
     "start_time": "2024-04-16T01:20:47.655168Z"
    }
   },
   "id": "4b85753bc04fd200",
   "execution_count": 59
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "bob = np.array([attribute_mat['lyr_name'][0][0]])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T02:20:45.352338Z",
     "start_time": "2024-04-16T02:20:45.336328Z"
    }
   },
   "id": "2a7da7e77c01ec65",
   "execution_count": 99
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<HDF5 object reference>]\n"
     ]
    }
   ],
   "source": [
    "print(bob)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T02:20:45.557747Z",
     "start_time": "2024-04-16T02:20:45.542976Z"
    }
   },
   "id": "4f39105f6fea0626",
   "execution_count": 100
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(1,)"
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bob.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T02:20:45.699081Z",
     "start_time": "2024-04-16T02:20:45.689221Z"
    }
   },
   "id": "58990852a21fd2e6",
   "execution_count": 101
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 object reference>\n",
      "<HDF5 dataset \"h\": shape (7, 1), type \"<u2\">\n",
      "115\n",
      "117\n",
      "114\n",
      "102\n",
      "97\n",
      "99\n",
      "101\n",
      "surface\n"
     ]
    }
   ],
   "source": [
    "print(bob[0])\n",
    "print(attribute_mat[bob[0]])\n",
    "\n",
    "# attribute_mat[bob[0]] is of type <HDF5 dataset \"h\": shape (7, 1), type \"<u2\">\n",
    "# print the contents of the dataset\n",
    "for i in range(7):\n",
    "    print(attribute_mat[bob[0]][i][0])\n",
    "\n",
    "# name is a list of the contents of the dataset\n",
    "deci_name = [attribute_mat[bob[0]][i][0] for i in range(7)]\n",
    "# convert name to a string using chr\n",
    "name = ''.join(chr(i) for i in deci_name)\n",
    "print(name)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T02:39:27.371016Z",
     "start_time": "2024-04-16T02:39:27.357454Z"
    }
   },
   "id": "5866961707a00885",
   "execution_count": 151
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surface\n"
     ]
    }
   ],
   "source": [
    "id = 0 # layer id number (0 or 1)\n",
    "\n",
    "decimal = attribute_mat[np.array([attribute_mat['lyr_name'][id][0]])[0]]\n",
    "decimal_name = [decimal[i][0] for i in range(len(decimal))]\n",
    "name = ''.join(chr(i) for i in decimal_name)\n",
    "print(name)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T02:46:35.464479Z",
     "start_time": "2024-04-16T02:46:35.452933Z"
    }
   },
   "id": "2a883fc948bd9788",
   "execution_count": 170
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f49194fe4eb12879"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
